# -*- coding: utf-8 -*-
"""GAFSP_webscraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IrCv6zCg_wIR5gRcTGQq8vzGI_UpvHyO
"""

import requests 
from bs4 import BeautifulSoup 
import re 
import os 
from urllib.request import unquote
import csv

# step 1: get to project page per year
def get_list_of_years():
  gafsp_url='https://www.gafspfund.org/projects' 
  response = requests.get(gafsp_url) 
  content = BeautifulSoup(response.text, 'lxml')

  l=content.find('div', class_='form-item js-form-item form-group form-type-select js-form-type-select form-item-call-for-proposal js-form-item-call-for-proposal form-no-label') 
  year_lines=l.find_all('option') 
  years=[] 
  for year_line in year_lines[1:]: 
    try: 
      year = re.findall('>.*<', str(year_line)) 
      if len(year)>0: 
        year=str(year)
        year=year.replace('[\'>', '').replace('<\']', '') 
        year=int(year)
        if year>=2015:
          years.append(year)
    except:
      pass
  return years

def get_project_documents(years):

  #concatenate each year to this link 
  url='https://www.gafspfund.org/projects?location=All&funding=All&supervising=All&field_project_status_target_id=All&call_for_proposal=' 
  url_end='&field_regional_value=0&page=' 
  page_number=0 
  max_pages=1

  for year in years: 
    while page_number<=max_pages: 
      projects_url=url+str(year)+url_end+str(page_number) 
      response = requests.get(projects_url) 
      content = BeautifulSoup(response.text, 'lxml') 
      l=content.find('div',class_='projects-grid mh') 
      if l: 
        links=l.find_all('a')
        # getting project links by calling the get_project_links function
        urls=get_project_links(links)
        # getting pdf document links under every project
        append_metadata(urls)
        # get_project_amount(urls)
        page_number=page_number+1
      else:
        page_number=0
        break
def get_project_links(links):

#save project links in urls 
  urls=[] 
  for link in links: 
    try: 
      project_link = re.findall('href=".+', str(link)) 
      if len(project_link)>0: 
        project_link=str(project_link) 
      project_link=project_link.replace('[\'href="/', '').replace('">\']','') 
      project_link='https://www.gafspfund.org/'+project_link 
      urls.append(project_link) 
      pass 
    except: 
        pass 
  return urls
def open_csv():
  with open('metadata.csv', 'w', newline='') as file:
      writer = csv.writer(file)
      writer.writerow(["project title", "country", "project status", "funding", "supervising entity", "call year", "project amount", "environmental category", "summary"])

def append_metadata(urls):
  with open('metadata.csv', 'a', newline='') as file:
      writer = csv.writer(file)
      for url in urls: 
        response = requests.get(url) 
        content = BeautifulSoup(response.text, 'lxml') 
        l=content.find('div', class_='info-card__field info-card__field-country') 
        l=l.find('li')
        l=str(l).strip('</li>')
        country=l.strip()
        l=content.find('div', class_="info-card__field info-card__field-project-status")
        l=l.find('span', class_='h3')
        l=l.text
        l=str(l)
        status=l.strip()[39:]
        l=content.find('div', class_='info-card__field info-card__field-funding') 
        try:
          l=l.find('span', class_='h3')
        except:
          l='None'
          pass
        if l!='None':
          l=l.text
          l=str(l)
          funding=l.strip()[32:]
        else:
          funding='Unknown'

        l=content.find('div', class_='info-card__field info-card__field-supervising-type') 
        l=l.find('span', class_='h3')
        l=l.text
        l=str(l)
        entity=l.strip().strip('Supervising entity').strip().replace("\t", '').replace("\n", '').replace('\r','')
        entity=re.sub(' +', ' ', entity)
        try:
            entity=entity.replace(' W', ' , W')
        except:
            pass
        try:
            entity=entity.replace(' I', ' , I')
        except:
          pass
        try: 
            entity=entity.replace(' F', ' , F')
        except:
          pass

        l=content.find('div', class_='info-card__field info-card__field-call-for-proposal-year') 
        l=l.find('span', class_='h3')
        l=l.text
        year=str(l).strip()[-4:]
      
        l=content.find('article', class_='node project project--promoted node--full project--full') 
        # l=l.find('p', class_='mb-0 mt-md-2')
        try:
          l=re.findall('.+million',str(l))
          l=str(l[0])[-14:]
          amount=l.partition('$')[2]
        except:
          amount='not found'

        l=content.find('div', class_='col-md-8') 
        summary=l.text

        l=content.find('h1', class_='h2 mt-2') 
        l=l.find('span')
        title=l.text
        writer.writerow([title, country,status,funding, entity, year, amount,'agriculture', summary])
def get_title(url):

    response = requests.get(url) 
    content = BeautifulSoup(response.text, 'lxml') 
    pdf_url = content.find_all('a')

    try:
      pdf_link = re.findall('/sites.+\.pdf', str(pdf_url))
      if len(pdf_link)>0:
        pdf_link=str(pdf_link)
        pdf_link=pdf_link.replace('[\'/','').replace('\']','')
        pdf_link='https://www.gafspfund.org/'+str(pdf_link)

    except:
      pass

    if len(pdf_link) > 0:
      pdf_response = requests.get(pdf_link)
      filename = (unquote(pdf_response.url).split('/')[-1]).strip('.pdf')
    
    return filename
    

def main(): 
  years=get_list_of_years() 
  open_csv()
  get_project_documents(years)

main()